#+OPTIONS: toc:nil 
# #+LaTeX_CLASS: koma-article 
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,aspectratio=169]
#+OPTIONS: H:2
#+LaTeX_HEADER: \usepackage{khpreamble}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \newcommand{\sign}{\mathrm{sign}}
#+LaTeX_HEADER: \renewcommand{\transp}{^{\mathrm{T}}}

#+title: Robust Kalman filter
#+date: 2019-11-13
* What do I want the students to understand? 			   :noexport:
** Convex optimization is fast and powerful
** How to formulate a convex opimization problem
** Robustify KF by 1-norm regularization

* What will the students do? 					   :noexport:
** Install cvx
** Modify their own kf
** Run tests with different values of regularization parameter


* Introduction
** Why a robust version of the Kalman filter?
** Why a robust version of the Kalman filter?
   The Kalman filter assumes Gaussian measurement noise and so it is very sensitive to outliers. 
** Example 1
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:
# Tracking in 2d 
    The target moves in a circle. Observations are noisy with one outlier
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:
   \begin{center}
   \includegraphics[width=0.9\linewidth]{circular-movement-crop}
   \end{center}
** Example 1 contd. 
   The model of the dynamics: /Nearly constant velocity model/
   \begin{equation*}
   x(k+1) = \bbm I & hI\\ 0 & I \ebm x(k) + \bbm \frac{h^2}{2} I\\hI\ebm v(k),
   \end{equation*}
   where the state vector contains the position and velocity of the target
   \[ x = \bbm p\\\dot{p} \ebm.\]
** Example 1 contd.
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:
   Result of tracking using standard Kalman filter
*** plot
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:
   \begin{center}
   \includegraphics[width=0.9\linewidth]{circular-movement-kf-crop}
   \end{center}
* Convex optimization
** Convex optimization
   \begin{center}
   \includegraphics[width=0.7\linewidth]{cvx-book.png}
   \end{center}

** Preparation example
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   Linear regression model
   \begin{equation*}
   y(k) = ax(k) + b + e(k) + w(k), 
   \end{equation*}
   where $e(k)$ is Gaussian noise and $w(k)$ is a sparse vector of outliers.
*** graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+BEGIN_CENTER 
     \includegraphics[width=\linewidth]{least_squares_example}
    #+END_CENTER

** Preparation example, contd
   Least squares estimation:
   \begin{equation*}
    \text{minimize} \; ||y - ax - b||_2\\
   \end{equation*}
   Or, equivalently
   \begin{align*}
    \text{minimize} \; & ||\epsilon||_2\\
    \text{subject to} \; & \epsilon = y - ax-b
   \end{align*}

** Preparation example, contd
   Least squares estimation:
   \begin{equation*}
    \text{minimize} \; ||y - ax - b||_2\\
   \end{equation*}
   Solved by forming 
   \[ A = \bbm x(1) & 1\\x(2) & 1\\ \vdots & \vdots\\ x(N) & 1\ebm \]
   and
   \[ z = \bbm a\\b\ebm, \]
   and solving for $z$ in the (over-determined) system of equations
   \[ Az = y. \]

** Preparation example, contd
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{equation*}
    \text{minimize} \; ||y - ax - b||_2\\
   \end{equation*}
*** graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+BEGIN_CENTER 
     \includegraphics[width=\linewidth]{least_squares_regression}
    #+END_CENTER

** The problem with least squares
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{align*}
    \text{minimize} \; &\sum_k \phi_{S}(\epsilon_k)\\
    \text{where} \; \phi_S(u) &= u^2
   \end{align*}
   
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

   #+BEGIN_LaTeX
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
            \end{axis}
          \end{tikzpicture}
        \end{center}
        
   #+END_LaTeX

** More robust: The Huber penalty function
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
    Also known as *robust least squares*
   \begin{align*}
    \text{minimize} \; &\sum_k \phi_{hub}(\epsilon_k)\\
    \text{where}\; \phi_{hub}(u) &= \begin{cases} u^2 & |u| \le M\\ M(2|u|-M) & |u| > M \end{cases}
   \end{align*}

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
   #+BEGIN_LaTeX
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
              \addplot[orange!90!black, thick, no marks, domain=-4:-1, samples=201] {2*abs(x)-1};
              \addplot[orange!90!black, thin, no marks, domain=-1:1, samples=201] {x^2};
              \addplot[orange!90!black, thick, no marks, domain=1:4, samples=201] {2*abs(x)-1};
            \end{axis}
          \end{tikzpicture}
        \end{center}
        
   #+END_LaTeX
** Preparation example: Robust least squares
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{equation*}
    \text{minimize} \; &\sum_k \phi_{hub}(\epsilon_k)\\
   \end{equation*}
*** graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+BEGIN_CENTER 
     \includegraphics[width=\linewidth]{robust_least_squares_regression}
    #+END_CENTER



* Robust Kalman Filter
** Robustifying the Kalman filter
** The measurement update of the Kalman filter
   We have the state space model
   \begin{align*}
   x(k+1) &= Fx(k) + v(k)\\
   y(k) &= Hx(k) + e(k) + z(k)\\
   e &\sim \mathcal{N}(0,R)\\
   v &\sim \mathcal{N}(0,Q)
   \end{align*}
   The measurement update of the Kalman filter can be shown to be equivalent to solving the problem
   \[ \text{minimize} \; (y-Hx)^{\mathrm{T}} R^{-1}(y-Hx) + (x-\hat{x}_{k|k-1})\transp P_{k|k-1}^{-1}(x-\hat{x}_{k|k-1}) \]
   The optimal solution is \( x^*=\hat{x}_{k|k} = \hat{x}_{k|k} + K(y-H\hat{x}_{k|k-a}),\) where $K$ is the Kalman gain.
** The measurement update of the Kalman filter
   Introduce $\tilde{x} = x - \hat{x}_{k|k-1}$ and $\tilde{y} = y - H\hat{x}_{k|k-1}$. 
   The minimization problem can then be written
   \begin{align*}
   \text{minimize} \quad  &(y-Hx)^{\mathrm{T}} R^{-1}(y-Hx) + (x-\hat{x}_{k|k-1})\transp P_{k|k-1}^{-1}(x-\hat{x}_{k|k-1})\\
     & \quad = (\tilde{y} -H\tilde{x})\transp R^{-1}(\tilde{y}-H\tilde{x}) + \tilde{x}\transp P_{k|k-1}^{-1}\tilde{x}
    \end{align*}
** The measurement update of the Kalman filter
   We now define the residuals $\epsilon$ for the system of equations
   # \[ \begin{bmatrix} Z_R(y-H\tilde{x})\\ Z_P \tilde{x} \end{bmatrix} = \epsilon\]
   \[ \begin{bmatrix} Z_R & 0\\0 & Z_P \end{bmatrix} \begin{bmatrix}(\tilde{y}-H\tilde{x})\\ \tilde{x} \end{bmatrix} = \epsilon,\]
   where $Z_R\transp Z_R = R^{-1}$ and $Z_P\transp Z_P = P_{k|k-1}^{-1}$. 

   The minimization problem can now be written
   \begin{align*}
   \text{minimize} \quad  & \epsilon\transp\epsilon \\
   \text{subject to} \quad & \begin{bmatrix} Z_R & 0\\0 & Z_P \end{bmatrix} \begin{bmatrix}(\tilde{y}-H\tilde{x})\\ \tilde{x} \end{bmatrix} = \epsilon,
    \end{align*}
    which is a least-squares problem.
** Robustifying the measurement update
   The idea is to use the Huber penalty function $\phi_{hub}$ instead of the quadratic criterion $\epsilon\transp \epsilon$.
   \begin{align*}
    \text{minimize} \quad & \sum_{i=1}^{n+m} \phi_{hub}\big( \epsilon(i)\big) \\
   \text{subject to} \quad & \begin{bmatrix} Z_R & 0\\0 & Z_P \end{bmatrix} \begin{bmatrix}(\tilde{y}-H\tilde{x})\\ \tilde{x} \end{bmatrix} = \epsilon,
    \end{align*}


** Tracking example again
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:   
10% chance of outlier with 10 times normal standard deviation

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:
   \begin{center}
   \includegraphics[width=0.9\linewidth]{circular_movement_rkf}
   \end{center}


* Old stuff							   :noexport:
** The measurement update of the Kalman filter
   Once the optimal solution $\tilde{x}^*$ and $e^*$ is found to the optimization problem
   \begin{align*}
    & \; = \begin{bmatrix}e\transp & \tilde{x}\transp \end{bmatrix} W \begin{bmatrix} e\\ \tilde{x} \end{bmatrix}\\
   & \tilde{y} = H\tilde{x} + e
   \end{align*}
   then the updated estimate is calculated as 
   \[ \hat{x}_{k|k} = \hat{x}_{k|k-1} + \tilde{x}^*\]
   the optimization problem has a closed-form solution $\tilde{x} = K\tilde{y} = K(y - Hx_{k|k-1})$, where $K$ is the Kalman gain. 
** The measurement update of the Kalman filter
   We have the state space model
   \begin{align*}
   x(k+1) &= Fx(k) + v(k)\\
   y(k) &= Hx(k) + e(k) + z(k)
   \end{align*}
   where
   \begin{align*}
   e &\sim \mathcal{N}(0,R)\\
   v &\sim \mathcal{N}(0,Q)
   \end{align*}
   The measurement update of the Kalman filter can be shown to be equivalent to solving the problem
   \[ \text{minimize} \; e^{\mathrm{T}} R^{-1} e + (x-\hat{x}_{k|k-1})\transp P_{k|k-1}^{-1}(x-\hat{x}_{k|k-1}) \]
   \[ \text{subject to}\quad y = Hx + e \]
   with variables $e$ and $x$.
** The measurement update of the Kalman filter
   The minimization problem can be written
   \begin{align*}
    \text{minimize} \quad & e\transp R^{-1} e + (x-\hat{x}_{k|k-1})\transp P_{k|k-1}^{-1}(x-\hat{x}_{k|k-1}) \\
    & \; = \begin{bmatrix}e\transp & (x-\hat{x}_{k|k-1})\transp \end{bmatrix} \begin{bmatrix} R^{-1} & 0\\0 &  P_{k|k-1}^{-1} \end{bmatrix} \begin{bmatrix} e\\x-\hat{x}_{k|k-1} \end{bmatrix}\\
    & \; = \begin{bmatrix}e\transp & \tilde{x}\transp \end{bmatrix} W \begin{bmatrix} e\\ \tilde{x} \end{bmatrix}\\
   \text{subject to}\quad & y = Hx + e  = H(\hat{x}_{k|k-1} + \tilde{x}) + e, \; \text{or}\\
   & \tilde{y} = H\tilde{x} + e
   \end{align*}
   with variables $e$ and $\tilde{x}$.
** The measurement update of the Kalman filter
   Once the optimal solution $\tilde{x}^*$ and $e^*$ is found to the optimization problem
   \begin{align*}
    & \; = \begin{bmatrix}e\transp & \tilde{x}\transp \end{bmatrix} W \begin{bmatrix} e\\ \tilde{x} \end{bmatrix}\\
   & \tilde{y} = H\tilde{x} + e
   \end{align*}
   then the updated estimate is calculated as 
   \[ \hat{x}_{k|k} = \hat{x}_{k|k-1} + \tilde{x}^*\]
   the optimization problem has a closed-form solution $\tilde{x} = K\tilde{y} = K(y - Hx_{k|k-1})$, where $K$ is the Kalman gain. 

** Kalman update alternative form
   The minization problem solved in the measurement update of the Kalman filter can be shown (next slide) to be equivalent to the problem
   \[ \text{minimize} \; (\tilde{y}-z)^{\mathrm{T}} W (\tilde{y}-z) \]
   with variable $z$. To compute $W$, first compute the Kalman gain
   \[ K = PH^{\mathrm{T}}(HPH^{\mathrm{T}} + R)^{-1}, \]
   and then
   \[ W = (I-HK)^{\mathrm{T}} R^{-1} (I-HK) + K^{\mathrm{T}} P^{-1} K. \]
   
   The update is finally computed as
   \[ x = \hat{x}_{k|k-1} + K(e-z) \]
** Obtaining the alternative form
   Start with the criterion 
   \[ \text{minimize} \; w^{\mathrm{T}} R^{-1} w + (x-\hat{x}_{k|k-1})P^{-1}(x-\hat{x}_{k|k-1})\]
   Substitute    \[ x = \hat{x}_{k|k-1} + K\tilde{y}, \]
   \[ e = y - Cx\] 
   and use the identity \[\tilde{y} = y-H\hat{x}_{k|k-1}.\] The alternative form follows.

*** Notes							   :noexport:
    
